{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "2f23aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "12e7b265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "\n",
    "import pennylane.numpy as pnp\n",
    "import pennylane as qml\n",
    "import jax\n",
    "from tqdm import tqdm\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from jax_utils import square_kernel_matrix_jax, kernel_matrix_jax, target_alignment_jax\n",
    "from pathlib import Path\n",
    "from torch_geometric.nn import knn_graph\n",
    "from torch_geometric.utils import to_undirected, k_hop_subgraph\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from itertools import zip_longest\n",
    "import networkx as nx\n",
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import torch\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "pnp.random.seed(seed)\n",
    "\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9946d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(pf, path, iter_batch_size, max_batches, zero_idx, one_idx, prefix):\n",
    "    record_batch = pf.iter_batches(batch_size=iter_batch_size)\n",
    "    count = 0\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            batch = next(record_batch)\n",
    "            zero_idx, one_idx = transform_to_graph(batch, path, zero_idx, one_idx, prefix)\n",
    "            count += 1\n",
    "            break\n",
    "        except StopIteration as e:\n",
    "            print(e)\n",
    "            return zero_idx, one_idx\n",
    "\n",
    "        if count == max_batches:\n",
    "            break\n",
    "\n",
    "    return zero_idx, one_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "93defc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_graph(batch, path, zero_idx, one_idx, prefix):\n",
    "    p = batch.to_pandas()\n",
    "    im = np.array(np.array(np.array(p.iloc[:, 0].tolist()).tolist()).tolist())\n",
    "    meta = np.array(p.iloc[:, 3])\n",
    "    return saver(im, meta, path, zero_idx, one_idx, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a469223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import pad\n",
    "from torch_geometric.utils import to_dense_batch, to_dense_adj\n",
    "from time import time\n",
    "\n",
    "max_nodes = 1024\n",
    "max_ego_nodes = 0\n",
    "\n",
    "def saver(im, meta, path, zero_idx, one_idx, prefix):\n",
    "\n",
    "    im[im < 1.e-3] = 0 #Zero_suppression\n",
    "    # im[:,0,:,:] = (im[:,0,:,:] - im[:,0,:,:].mean())/(im[:,0,:,:].std())\n",
    "    # im[:,1,:,:] = (im[:,1,:,:] - im[:,1,:,:].mean())/(im[:,1,:,:].std())\n",
    "    # im[:,2,:,:] = (im[:,2,:,:] - im[:,2,:,:].mean())/(im[:,2,:,:].std())\n",
    "\n",
    "    new_file = True\n",
    "    \n",
    "    with tqdm(range(meta.shape[0]), unit='datum') as tbatch:   \n",
    "        for i in tbatch:\n",
    "            img = im[i,:,:,:]\n",
    "            label = int(meta[i])\n",
    "\n",
    "#             channel1 = img[0,:,:]\n",
    "#             channel2 = img[1,:,:]\n",
    "#             channel3 = img[2,:,:]\n",
    "\n",
    "#             channel1 = np.clip(channel1, 0, 500*channel1.std())\n",
    "#             channel2 = np.clip(channel2, 0, 500*channel2.std())\n",
    "#             channel3 = np.clip(channel3, 0, 500*channel3.std())\n",
    "\n",
    "#             p = channel1.max() == 0.0\n",
    "#             q = channel2.max() == 0.0\n",
    "#             r = channel3.max() == 0.0\n",
    "\n",
    "#             if p | q | r:\n",
    "#                 continue\n",
    "\n",
    "#             channel1 = channel1/channel1.max()\n",
    "#             channel2 = channel2/channel2.max()\n",
    "#             channel3 = channel3/channel3.max()\n",
    "\n",
    "#             img[0,:,:] = channel1\n",
    "#             img[1,:,:] = channel2\n",
    "#             img[2,:,:] = channel3\n",
    "\n",
    "            img = img.T\n",
    "\n",
    "            # graph conversion\n",
    "            img = torch.Tensor(img)\n",
    "            xhit, yhit, zhit = torch.nonzero(img, as_tuple=True)\n",
    "            \n",
    "            # print(xhit.shape, yhit.shape, zhit.shape)\n",
    "            \n",
    "            # indices\n",
    "            chs = [(zhit == 0).nonzero(as_tuple=True)[0], \n",
    "                   (zhit == 1).nonzero(as_tuple=True)[0], \n",
    "                   (zhit == 2).nonzero(as_tuple=True)[0]]\n",
    "\n",
    "            hcal_coordinates = torch.stack((xhit[chs[2]], yhit[chs[2]])).T\n",
    "            hcal_values =  img[xhit[chs[2]], yhit[chs[2]], 2]\n",
    "            \n",
    "            assert hcal_coordinates.shape[0]%5 == 0\n",
    "            \n",
    "            # print(hcal_indices.shape)\n",
    "            # Reshape the padded array into groups of 5 elements\n",
    "            grouped_coordinates = hcal_coordinates.reshape(-1, 5, 2).to(torch.float32)\n",
    "            grouped_values = hcal_values.reshape(-1, 5)\n",
    "#             print(grouped_indices)\n",
    "#             print(grouped_values)\n",
    "            \n",
    "            # Calculate the mean along the second axis (axis=1)\n",
    "            mean_coordinates = torch.mean(grouped_coordinates, axis=1).numpy()\n",
    "#             print(mean_indices)\n",
    "            mean_values = torch.mean(grouped_values, axis=1)\n",
    "#             print(mean_values)\n",
    "            \n",
    "            mean_coordinates = mean_coordinates[np.lexsort((mean_coordinates[:, 0], mean_coordinates[:, 1]))]\n",
    "            \n",
    "#             print(mean_indices)\n",
    "            \n",
    "            mean_coordinates = torch.tensor(mean_coordinates)\n",
    "        \n",
    "            grouped_coordinates = mean_coordinates.reshape(-1, 5, 2).to(torch.float32)\n",
    "            grouped_values = mean_values.reshape(-1, 5)\n",
    "#             print(grouped_indices)\n",
    "#             print(grouped_values)\n",
    "            \n",
    "            mean_coordinates = torch.mean(grouped_coordinates, axis=1).to(torch.int)\n",
    "            mean_values = torch.mean(grouped_values, axis=1)\n",
    "            # print(mean_indices.shape)\n",
    "            # print(mean_values.shape)\n",
    "            \n",
    "            xhit = torch.cat((xhit[chs[0]], xhit[chs[1]], mean_coordinates[:,0])).to(torch.int)\n",
    "            yhit = torch.cat((yhit[chs[0]], yhit[chs[1]], mean_coordinates[:,1])).to(torch.int)\n",
    "            zhit = torch.cat((zhit[chs[0]], zhit[chs[1]], torch.ones(mean_coordinates.shape[0])*2)).to(torch.int)\n",
    "            \n",
    "            non_zero_values = img[xhit, yhit, zhit]\n",
    "            non_zero_values *= 50\n",
    "            \n",
    "            xhit = xhit.to(torch.float32)\n",
    "            yhit = yhit.to(torch.float32)\n",
    "            zhit = zhit.to(torch.float32)\n",
    "            \n",
    "            zhit[zhit == 0] = 3\n",
    "            zhit[zhit == 1] = 5.5\n",
    "            zhit[zhit == 2] = 8.5\n",
    "            \n",
    "            node_feats = torch.stack((xhit, yhit, zhit, non_zero_values), dim=1)\n",
    "            node_feats = node_feats[:max_nodes]\n",
    "            coords = node_feats[:, [0, 1, 2]]\n",
    "            \n",
    "            if len(coords) == 0:\n",
    "                continue\n",
    "\n",
    "            # Create knn graph adjacency matrix\n",
    "            edge_index = knn_graph(coords,\n",
    "                                   k=6,\n",
    "                                   batch=None,\n",
    "                                   loop=True,\n",
    "                                   num_workers=16)\n",
    "            \n",
    "            \n",
    "            # generate ego graphs\n",
    "            n_nodes = node_feats.shape[0]\n",
    "            ego_nodes = []\n",
    "            k = 5\n",
    "            \n",
    "#             edge_index = to_undirected(edge_index)\n",
    "            global max_ego_nodes\n",
    "            for node in range(n_nodes):\n",
    "                try:\n",
    "                    subset, sub_edge_index, _, _ = k_hop_subgraph(node,\n",
    "                                                              k,\n",
    "                                                              edge_index,\n",
    "                                                              directed=False)\n",
    "                except:\n",
    "                    subset = []\n",
    "                    sub_edge_index = torch.tensor([])\n",
    "                    \n",
    "                n_subset_nodes = len(subset)\n",
    "                \n",
    "                if n_subset_nodes:\n",
    "                    G = nx.Graph()\n",
    "                    G.add_edges_from(sub_edge_index.numpy().T)\n",
    "                    paths = nx.single_source_shortest_path_length(G, node, cutoff=k)\n",
    "\n",
    "                    nodes = np.array(list(paths.keys()))\n",
    "                    dists = np.array(list(paths.values()))\n",
    "\n",
    "                    hop_nodes = [\n",
    "                        [node] + list(nodes[np.where(dists == hop)[0]]) for hop in range(1, k + 1)\n",
    "                    ]\n",
    "\n",
    "                    hop_nodes = np.array(list(zip_longest(*hop_nodes, fillvalue=max_nodes+1))).T\n",
    "                else:\n",
    "                    dists = np.array([])\n",
    "                    hop_nodes = np.array([np.array([])]*k)\n",
    "                    \n",
    "                ego_nodes.append(hop_nodes)\n",
    "\n",
    "                max_ego_nodes = max(max_ego_nodes, hop_nodes.shape[-1])\n",
    "            \n",
    "                \n",
    "#             print(node_feats.shape)\n",
    "#             print(edge_index.shape)\n",
    "\n",
    "#             dense_node_feats, node_mask = to_dense_batch(node_feats, max_num_nodes=max_nodes)\n",
    "#             dense_adj = to_dense_adj(edge_index, edge_attr=None, max_num_nodes=max_nodes)\n",
    "            \n",
    "            # print(node_feats.shape)\n",
    "            # print(dense_node_feats.shape)\n",
    "            \n",
    "            # print(dense_node_feats[0].shape, node_mask[0].shape)\n",
    "            # print(dense_adj[0].shape)\n",
    "#             print('-'*50)\n",
    "            \n",
    "            # num_nodes.append(non_zero_values.shape[0])\n",
    "            # num_edges.append(edge_index.shape[1])\n",
    "            \n",
    "            # if label == 0:\n",
    "            #     save_path = path / 'quark'\n",
    "            #     # np.savez_compressed(save_path / str(zero_idx), \n",
    "            #     #                     node_feats=node_feats, \n",
    "            #     #                     edge_index=edge_index,\n",
    "            #     #                    )\n",
    "            #     np.savez_compressed(save_path / str(zero_idx), \n",
    "            #                         node_feats=dense_node_feats[0], \n",
    "            #                         node_mask=node_mask[0],\n",
    "            #                         adj=dense_adj[0]\n",
    "            #                        )\n",
    "            #     zero_idx += 1\n",
    "            # else:\n",
    "            #     save_path = path / 'gluon'\n",
    "            #     np.savez_compressed(save_path / str(one_idx), \n",
    "            #                         node_feats=dense_node_feats[0], \n",
    "            #                         node_mask=node_mask[0],\n",
    "            #                         adj=dense_adj[0]\n",
    "            #                        )\n",
    "            #     one_idx += 1\n",
    "            \n",
    "            \n",
    "            # sparse\n",
    "#             parquet_df = pd.DataFrame({\n",
    "#                 'x': [node_feats[:, 0].numpy()],\n",
    "#                 'y': [node_feats[:, 1].numpy()],\n",
    "#                 'detector': [node_feats[:, 2].numpy()],\n",
    "#                 'energy': [node_feats[:, 3].numpy()],\n",
    "#                 'edge_index_from': [edge_index[0, :].numpy()],\n",
    "#                 'edge_index_to': [edge_index[1, :].numpy()],\n",
    "#                 'y': [label],\n",
    "#             })\n",
    "\n",
    "#             table = pa.Table.from_pandas(parquet_df)\n",
    "#             if new_file:\n",
    "#                 output_filename = Path(path) / f'{prefix}_{zero_idx}.parquet'\n",
    "#                 pqwriter = pq.ParquetWriter(output_filename, table.schema, compression='snappy')\n",
    "#                 new_file = False\n",
    "#                 zero_idx += 1\n",
    "#             pqwriter.write_table(table)\n",
    "    \n",
    "            \n",
    "    return zero_idx, one_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c3ae911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir = Path('./processed_qg_parquets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b24fdb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['qg_train_0.parquet']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(processed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b5e79a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = ['QCDToGGQQ_IMGjet_RH1all_jet0_run0_n36272.test.snappy.parquet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2b894c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: QCDToGGQQ_IMGjet_RH1all_jet0_run0_n36272.test.snappy.parquet 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 1024/1024 [04:44<00:00,  3.60datum/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files were successfully generated\n"
     ]
    }
   ],
   "source": [
    "zero_idx = 0\n",
    "one_idx = 0\n",
    "prefix = 'qg_train'\n",
    "iter_batch_size = 1024\n",
    "max_batches = -1\n",
    "\n",
    "for raw_path in train_files:\n",
    "    print(\"Processing file:\", raw_path, zero_idx, one_idx)\n",
    "    zero_idx, one_idx = generate(pq.ParquetFile(raw_path), processed_dir,\n",
    "                                 iter_batch_size, max_batches, zero_idx,\n",
    "                                 one_idx, prefix)\n",
    "\n",
    "print(\"The files were successfully generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5651695f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_ego_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3e3fc2",
   "metadata": {},
   "source": [
    "Tested with 1000 qg samples\n",
    "\n",
    "k qubits\n",
    "1 6\n",
    "\n",
    "2 23\n",
    "\n",
    "3 41\n",
    "\n",
    "4 47\n",
    "\n",
    "5 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd7806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qmlhep)",
   "language": "python",
   "name": "qmlhep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
